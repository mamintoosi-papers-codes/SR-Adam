\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{stein1956}
\citation{james1961}
\citation{saleh2006}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries: The Estimator Framework}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Proposed Method: The Stein-Rule Gradient}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}The Test Statistic}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}The Stein-Rule Estimator}{2}{subsection.3.2}\protected@file@percent }
\newlabel{eq:pos_stein}{{\mbox  {III-B}}{2}{The Stein-Rule Estimator}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Adaptive Variance Estimation in Adam}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}The SR-Adam Algorithm}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experimental Results}{2}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Model Architecture}{2}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}SR-Adam Application Strategy}{2}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SR-Adam: Stein-Rule Adaptive Moment Estimation}}{3}{algorithm.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces SimpleCNN architecture: layer configuration, output shape, and parameters.}}{3}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:simplecnn_arch}{{I}{3}{SimpleCNN architecture: layer configuration, output shape, and parameters}{table.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {VI-A}1a}Why Stein-Rule on Convolutions Only?}{3}{paragraph.6.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}Experimental Setup and Results}{3}{subsection.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces SR-Adam parameter grouping in SimpleCNN: Stein-rule applied only to convolutional layer weights (3.4\% of total parameters).}}{4}{table.caption.2}\protected@file@percent }
\newlabel{tab:sradam_grouping}{{II}{4}{SR-Adam parameter grouping in SimpleCNN: Stein-rule applied only to convolutional layer weights (3.4\% of total parameters)}{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces SimpleCNN on CIFAR10: test accuracy vs.\ epoch across noise levels. Mean $\pm $ std over runs.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:cifar10_acc}{{1}{4}{SimpleCNN on CIFAR10: test accuracy vs.\ epoch across noise levels. Mean $\pm $ std over runs}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {VI-B}0a}Fairness and Reproducibility}{4}{paragraph.6.2.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}1}Per-Epoch Behavior}{4}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}2}Aggregated Metrics}{4}{subsubsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}3}Qualitative Analysis: Prediction Visualization}{4}{subsubsection.6.2.3}\protected@file@percent }
\bibcite{stein1956}{1}
\bibcite{james1961}{2}
\bibcite{saleh2006}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SimpleCNN on CIFAR10: test loss vs.\ epoch across noise levels. Mean $\pm $ std over runs.}}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cifar10_loss}{{2}{5}{SimpleCNN on CIFAR10: test loss vs.\ epoch across noise levels. Mean $\pm $ std over runs}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces SimpleCNN on CIFAR100: test accuracy vs.\ epoch across noise levels. Mean $\pm $ std over runs.}}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:cifar100_acc}{{3}{5}{SimpleCNN on CIFAR100: test accuracy vs.\ epoch across noise levels. Mean $\pm $ std over runs}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Discussion and Conclusion}{5}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{5}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SimpleCNN on CIFAR100: test loss vs.\ epoch across noise levels. Mean $\pm $ std over runs.}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:cifar100_loss}{{4}{6}{SimpleCNN on CIFAR100: test loss vs.\ epoch across noise levels. Mean $\pm $ std over runs}{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Best test accuracy (mean $\pm $ std) over epochs; higher is better.}}{6}{table.caption.7}\protected@file@percent }
\newlabel{tab:method_best_acc}{{III}{6}{Best test accuracy (mean $\pm $ std) over epochs; higher is better}{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Final test accuracy (mean $\pm $ std) at last epoch; higher is better.}}{6}{table.caption.8}\protected@file@percent }
\newlabel{tab:method_final_acc}{{IV}{6}{Final test accuracy (mean $\pm $ std) at last epoch; higher is better}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Best test loss (mean $\pm $ std) over epochs; lower is better.}}{6}{table.caption.9}\protected@file@percent }
\newlabel{tab:method_best_loss}{{V}{6}{Best test loss (mean $\pm $ std) over epochs; lower is better}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Final test loss (mean $\pm $ std) at last epoch; lower is better.}}{6}{table.caption.10}\protected@file@percent }
\newlabel{tab:method_final_loss}{{VI}{6}{Final test loss (mean $\pm $ std) at last epoch; lower is better}{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Qualitative comparison on CIFAR10 with 5\% label noise. Each column shows a test image with its true label (top), Adam prediction (middle), and SR-Adam prediction (bottom). Predictions include class name and confidence score. Green indicates correct classification; red indicates misclassification. The accuracies in parentheses (74.7\% for Adam, 76.2\% for SR-Adam) represent the best single run out of 5 independent runs, selected for visualization clarity. Note that these values differ from the aggregated statistics in Table \ref {tab:method_best_acc} (73.95±0.44\% for Adam, 75.84±0.31\% for SR-Adam), which report mean ± std across all runs. SR-Adam demonstrates more confident and accurate predictions on challenging samples, particularly on visually similar classes like cats/dogs and automobiles/trucks.}}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig:qualitative_cifar10}{{5}{7}{Qualitative comparison on CIFAR10 with 5\% label noise. Each column shows a test image with its true label (top), Adam prediction (middle), and SR-Adam prediction (bottom). Predictions include class name and confidence score. Green indicates correct classification; red indicates misclassification. The accuracies in parentheses (74.7\% for Adam, 76.2\% for SR-Adam) represent the best single run out of 5 independent runs, selected for visualization clarity. Note that these values differ from the aggregated statistics in Table \ref {tab:method_best_acc} (73.95±0.44\% for Adam, 75.84±0.31\% for SR-Adam), which report mean ± std across all runs. SR-Adam demonstrates more confident and accurate predictions on challenging samples, particularly on visually similar classes like cats/dogs and automobiles/trucks}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Qualitative comparison on CIFAR100 with 5\% label noise. Similar to Figure \ref {fig:qualitative_cifar10}, this visualization shows sample predictions from the best-performing checkpoints. CIFAR100's 100-class taxonomy presents a significantly harder recognition task than CIFAR10. The fine-grained categories (e.g., distinguishing between different tree species or vehicle types) require more nuanced feature learning. SR-Adam's shrinkage mechanism helps stabilize gradient estimates in this high-noise, high-complexity regime, leading to more robust decision boundaries. The confidence scores reveal that SR-Adam produces higher-confidence predictions on correctly classified samples, suggesting improved calibration and reduced uncertainty.}}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:qualitative_cifar100}{{6}{7}{Qualitative comparison on CIFAR100 with 5\% label noise. Similar to Figure \ref {fig:qualitative_cifar10}, this visualization shows sample predictions from the best-performing checkpoints. CIFAR100's 100-class taxonomy presents a significantly harder recognition task than CIFAR10. The fine-grained categories (e.g., distinguishing between different tree species or vehicle types) require more nuanced feature learning. SR-Adam's shrinkage mechanism helps stabilize gradient estimates in this high-noise, high-complexity regime, leading to more robust decision boundaries. The confidence scores reveal that SR-Adam produces higher-confidence predictions on correctly classified samples, suggesting improved calibration and reduced uncertainty}{figure.caption.12}{}}
\gdef \@abspage@last{7}

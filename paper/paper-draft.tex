\documentclass[journal, onecolumn]{IEEEtran}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage[colorlinks, citecolor=magenta]{hyperref}


\title{Shrinkage Estimation in High-Dimensional Deep Learning: A Stein-Rule Approach to Stochastic Optimization}

\author{M.Arashi\footnote{Corresponding author. Email: arashi@um.ac.ir} \\
\textit{Department of Statistics, Faculty of Mathematical Sciences, Ferdowsi University of Mashhad, P. O. Box 1159, Mashhad 91775, Iran}}

\begin{document}

\maketitle
 
\begin{abstract}
Deep neural networks operate in parameter spaces of dimension $p \gg 3$, a regime where the classical Maximum Likelihood Estimator (MLE) is known to be inadmissible under quadratic loss. Despite this, standard Stochastic Gradient Descent (SGD) treats the mini-batch gradient as an unbiased, unrestricted estimator of the true gradient. In this paper, we bridge the gap between classical shrinkage estimation and modern deep learning. We propose a Stein-Rule Gradient Estimator that shrinks the noisy stochastic gradient toward a stable Restricted Estimator (the historical momentum) based on a dynamic hypothesis test of signal quality. We further introduce an adaptive noise variance estimator derived from the second moments of the Adam optimizer. The resulting algorithm, Stein-Rule Adam (SR-Adam), theoretically guarantees lower asymptotic distributional risk in gradient estimation, offering a rigorous statistical alternative to heuristic regularization.
\end{abstract}

\section{Experimental Results}
We present a controlled empirical study designed to isolate the optimizer's effect while keeping the rest of the training pipeline fixed. Experiments span CIFAR10 and CIFAR100 with three levels of label noise (0.0, 0.05, 0.1). Unless otherwise stated, the optimizer is the only varying component; backbone, preprocessing, epochs, batch size, and evaluation are held constant to ensure a fair comparison. We first summarize the backbone and the SR-Adam application strategy, then report per-epoch behavior and aggregated metrics across runs.

\subsection{Model Architecture}
We employ a lightweight SimpleCNN backbone for all experiments. Table \ref{tab:simplecnn_arch} details the architecture, comprising two convolutional layers (32 and 64 filters with 3×3 kernels), followed by two fully connected layers (128 and 10 units). The total parameter count is 545,098, which keeps the model compact and allows us to focus on the optimizer's behavior without architectural confounds.

\input{simplecnn_arch_content.tex}

\subsubsection{SR-Adam Application Strategy}
SR-Adam applies Stein-rule shrinkage selectively: only to the convolutional layers where high-dimensional feature maps and noisy batch gradients make shrinkage estimation particularly valuable. Table \ref{tab:sradam_grouping} shows that SR-Adam manages 18,528 parameters (3.4\% of the total) in Conv2d modules, while fully-connected layers use standard Adam updates. This selective grouping exploits the Stein-rule's strength in high-dimensional regimes while preserving the direct adaptive behavior for low-dimensional projection layers.

\input{sradam_grouping_content.tex}

\subsection{Experimental Setup and Results}
We evaluate SR-Adam against SGD, Momentum, and Adam on CIFAR10 and CIFAR100 using the SimpleCNN backbone. To probe robustness to label noise, we add corruptions at three levels: 0.0, 0.05, and 0.1. For each dataset/noise combination, we conduct 5 independent runs with different random seeds and report mean $\pm$ std across runs. Accuracy is reported as a percentage (higher is better); loss is minimized (lower is better). In method comparison tables (Tables \ref{tab:method_best_acc}--\ref{tab:method_final_loss}), we bold the best entry per dataset/noise column according to the respective metric direction.

\paragraph{Fairness and Reproducibility}
All comparisons use an identical training protocol and data processing across methods; only the optimizer changes. We hold constant the backbone, number of epochs, batch size, label-noise injection, evaluation procedure, and seed schedule (five seeds per configuration), while using standard, fixed hyperparameters per optimizer throughout. To avoid discrepancies due to third-party implementations, all optimizers are implemented within a single, consistent codebase with matching interfaces, and the full executable code is publicly available from the paper repository
\footnote{\url{https://github.com/mamintoosi-papers-codes/SR-Adam}}.

\subsubsection{Per-Epoch Behavior}
Figures \ref{fig:cifar10_acc}--\ref{fig:cifar100_loss} display test accuracy and loss across epochs, stratified by noise level. Each figure shows three panels corresponding to noise levels 0.0, 0.05, and 0.1. All four methods (SGD, Momentum, Adam, SR-Adam) are plotted with mean ± std bands.

\input{experimental_figures.tex}

\subsubsection{Aggregated Metrics}
Tables \ref{tab:method_best_acc} and \ref{tab:method_best_loss} report the best accuracy and loss achieved over all epochs; Tables \ref{tab:method_final_acc} and \ref{tab:method_final_loss} report final epoch values. Each table rows are methods and columns are dataset×noise combinations. The mean $\pm$ std are computed across the 5 runs; bolded entries highlight the best-performing method per column. SR-Adam consistently achieves competitive or superior performance, particularly in high-noise regimes (0.05, 0.1), demonstrating the benefit of dynamic shrinkage on noisy gradients.

\input{minimal-tables-content.tex}

\subsubsection{Qualitative Analysis: Prediction Visualization}
To complement the quantitative metrics, we present a visual comparison of model predictions at noise level 0.05. Figures \ref{fig:qualitative_cifar10} and \ref{fig:qualitative_cifar100} display sample test images along with predictions from both Adam and SR-Adam using their respective best-performing checkpoints (highest test accuracy across 5 runs).

Each figure shows 10 randomly sampled test images arranged in columns. The top row displays the ground truth label, while the middle and bottom rows show Adam and SR-Adam predictions with their confidence scores. Correct predictions are marked in green; incorrect ones in red. The accuracies shown in parentheses correspond to the single best run selected for visualization.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{qualitative_cifar10_noise005.pdf}
    \caption{Qualitative comparison on CIFAR10 with 5\% label noise. Each column shows a test image with its true label (top), Adam prediction (middle), and SR-Adam prediction (bottom). Predictions include class name and confidence score. Green indicates correct classification; red indicates misclassification. The accuracies in parentheses (74.7\% for Adam, 76.2\% for SR-Adam) represent the best single run out of 5 independent runs, selected for visualization clarity. Note that these values differ from the aggregated statistics in Table \ref{tab:method_best_acc} (73.95±0.44\% for Adam, 75.84±0.31\% for SR-Adam), which report mean ± std across all runs. SR-Adam demonstrates more confident and accurate predictions on challenging samples, particularly on visually similar classes like cats/dogs and automobiles/trucks.}
    \label{fig:qualitative_cifar10}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{qualitative_cifar100_noise005.pdf}
    \caption{Qualitative comparison on CIFAR100 with 5\% label noise. Similar to Figure \ref{fig:qualitative_cifar10}, this visualization shows sample predictions from the best-performing checkpoints. CIFAR100's 100-class taxonomy presents a significantly harder recognition task than CIFAR10. The fine-grained categories (e.g., distinguishing between different tree species or vehicle types) require more nuanced feature learning. SR-Adam's shrinkage mechanism helps stabilize gradient estimates in this high-noise, high-complexity regime, leading to more robust decision boundaries. The confidence scores reveal that SR-Adam produces higher-confidence predictions on correctly classified samples, suggesting improved calibration and reduced uncertainty.}
    \label{fig:qualitative_cifar100}
\end{figure*}

These visualizations reveal several key insights: (1) SR-Adam tends to produce higher confidence scores on correctly classified samples, suggesting better calibration; (2) on misclassified samples, both methods often confuse visually similar classes (e.g., cats vs dogs, trucks vs automobiles), but SR-Adam demonstrates slightly better discrimination; (3) the shrinkage mechanism appears particularly beneficial for challenging, ambiguous samples where the raw gradient signal is noisy. While the quantitative improvement is modest (approximately 2 percentage points), the qualitative analysis shows that SR-Adam's adaptive shrinkage translates to more confident and stable predictions in practice.


\end{document}
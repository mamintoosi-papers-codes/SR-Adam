\documentclass[journal, onecolumn]{IEEEtran}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}

\title{Shrinkage Estimation in High-Dimensional Deep Learning: A Stein-Rule Approach to Stochastic Optimization}

\author{M.Arashi\footnote{Corresponding author. Email: arashi@um.ac.ir} \\
\textit{Department of Statistics, Faculty of Mathematical Sciences, Ferdowsi University of Mashhad, P. O. Box 1159, Mashhad 91775, Iran}}

\begin{document}

\maketitle
 
\begin{abstract}
Deep neural networks operate in parameter spaces of dimension $p \gg 3$, a regime where the classical Maximum Likelihood Estimator (MLE) is known to be inadmissible under quadratic loss. Despite this, standard Stochastic Gradient Descent (SGD) treats the mini-batch gradient as an unbiased, unrestricted estimator of the true gradient. In this paper, we bridge the gap between classical shrinkage estimation and modern deep learning. We propose a Stein-Rule Gradient Estimator that shrinks the noisy stochastic gradient toward a stable Restricted Estimator (the historical momentum) based on a dynamic hypothesis test of signal quality. We further introduce an adaptive noise variance estimator derived from the second moments of the Adam optimizer. The resulting algorithm, Stein-Rule Adam (SR-Adam), theoretically guarantees lower asymptotic distributional risk in gradient estimation, offering a rigorous statistical alternative to heuristic regularization.
\end{abstract}
\section{Introduction}
The central tenet of classical frequentist statistics, established by Stein (1956) \cite{stein1956} and refined by James and Stein (1961) \cite{james1961}, is that in dimensions $p \ge 3$, the ordinary Maximum Likelihood Estimator (MLE) is inadmissible. That is, there exists a biased estimator, termed the shrinkage estimator, that achieves a strictly lower Mean Squared Error (MSE) universally across the parameter space. Modern Deep Learning (DL) is inherently a high-dimensional estimation problem, often exceeding $p = 10^6$. Yet, the dominant optimization paradigm, Stochastic Gradient Descent (SGD), relies on the unbiased estimation of the gradient vector $\nabla \mathcal{L}(\boldsymbol{\theta})$ via mini-batches. While unbiasedness is a desirable property in low dimensions, in high dimensions it subjects the optimization trajectory to excessive variance, leading to slow convergence and poor generalization.
Current solutions to this variance problem, such as momentum or weight decay ($L_2$ regularization), are heuristic. Momentum is a linear smoothing technique, while weight decay imposes a static zero-mean prior. Neither approach adapts dynamically to the statistical significance of the gradient signal.
In this work, we formalize the neural network optimization step as a \textit{point estimation problem}. We apply the theory of Preliminary Test (PT) and Stein-Rule (SR) estimation \cite{saleh2006} to construct a gradient estimator that dynamically trades bias for variance reduction. We demonstrate that the Adam optimizer's internal state provides sufficient statistics to construct an adaptive shrinkage factor, leading to the proposed \textbf{SR-Adam} algorithm.
\section{Preliminaries: The Estimator Framework}
Let $\boldsymbol{\theta} \in \mathbb{R}^p$ denote the parameter vector of a neural network, where $p$ is large. We aim to minimize a loss function $J(\boldsymbol{\theta}) = \mathbb{E}_{x \sim \mathcal{D}}[L(x, \boldsymbol{\theta})]$.
At time step $t$, we observe a mini-batch gradient $\mathbf{g}_t$, which we treat as the \textit{Unrestricted Estimator} (UE) of the true gradient $\nabla J(\boldsymbol{\theta}_t)$:
$$    \mathbf{g}_t = \nabla J(\boldsymbol{\theta}_t) + \boldsymbol{\epsilon}_t, \quad \boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_p)$$
Standard SGD uses $\mathbf{g}_t$ directly to update $\boldsymbol{\theta}$.
We posit the existence of a \textit{Restricted Estimator} (RE), $\tilde{\mathbf{g}}_t$, which embodies prior knowledge or stability. In the context of optimization with momentum, the exponential moving average of past gradients serves as a natural proxy for the "stable" direction of descent:
$$    \tilde{\mathbf{g}}_t = \mathbf{m}_{t-1}$$
where $\mathbf{m}_{t-1}$ is the first moment estimate from the previous step.
The objective is to find an estimator $\hat{\mathbf{g}}_t$ that minimizes the weighted quadratic risk:
$$    R(\hat{\mathbf{g}}_t) = \mathbb{E} \left[ || \hat{\mathbf{g}}_t - \nabla J(\boldsymbol{\theta}_t) ||^2 \right]$$
\section{Proposed Method: The Stein-Rule Gradient}
Under the James-Stein framework, we construct a shrinkage estimator $\mathbf{g}^{S}_t$ that shrinks the high-variance UE ($\mathbf{g}_t$) towards the low-variance RE ($\mathbf{m}_{t-1}$).
\subsection{The Test Statistic}
We define the test statistic $D_n$ based on the squared Euclidean distance between the current observation and the historical trend:
$$    D_n = || \mathbf{g}_t - \mathbf{m}_{t-1} ||^2_2$$
This statistic essentially tests the hypothesis $H_0: \nabla J(\boldsymbol{\theta}_t) = \mathbf{m}_{t-1}$. A large $D_n$ suggests the current batch gradient deviates significantly from the established trajectory (high noise or distributional shift).
\subsection{The Stein-Rule Estimator}
The Stein-Rule estimator for the gradient is defined as:
$$    \mathbf{g}^{S}_t = \mathbf{m}_{t-1} + \left( 1 - \frac{(p-2)\sigma^2}{D_n} \right) (\mathbf{g}_t - \mathbf{m}_{t-1})$$
To ensure the shrinkage factor remains valid (non-negative), we adopt the Positive-Rule Stein Estimator:
$$    \label{eq:pos_stein}
    \mathbf{g}^{S+}_t = \mathbf{m}_{t-1} + \left[ 1 - \frac{(p-2)\sigma^2}{|| \mathbf{g}_t - \mathbf{m}_{t-1} ||^2} \right]^+ (\mathbf{g}_t - \mathbf{m}_{t-1})$$
where $[z]^+ = \max(0, z)$.
\section{Adaptive Variance Estimation in Adam}
A critical challenge in applying Eq. (\ref{eq:pos_stein}) is the unknown noise variance $\sigma^2$. In static regression, this is estimated via residual sum of squares. In stochastic optimization, we must estimate it online.
The Adam optimizer maintains running estimates of the first moment $\mathbf{m}_t \approx \mathbb{E}[\mathbf{g}]$ and the second raw moment $\mathbf{v}_t \approx \mathbb{E}[\mathbf{g}^2]$. Using the identity $\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$, we can approximate the element-wise variance of the gradient at step $t$:
$$    \hat{\boldsymbol{\sigma}}^2_t = \mathbf{v}_t - \mathbf{m}_t^2$$
To obtain a scalar variance estimate $\hat{\sigma}^2$ for the Stein correction (which assumes homoscedasticity across the layer for stability), we average over the dimension $p$:
$$    \hat{\sigma}^2_{global} = \frac{1}{p} \sum_{j=1}^p \left( [\mathbf{v}_t]_j - ([\mathbf{m}_t]_j)^2 \right)$$
Substituting $\hat{\sigma}^2_{global}$ into Eq. (\ref{eq:pos_stein}) yields a fully adaptive, hyperparameter-free shrinkage mechanism.
\section{The SR-Adam Algorithm}
We formally present the Stein-Rule Adam (SR-Adam) algorithm.
\begin{algorithm}
\caption{SR-Adam: Stein-Rule Adaptive Moment Estimation}
\begin{algorithmic}[1]
\REQUIRE $\alpha$: Learning rate
\REQUIRE $\beta_1, \beta_2 \in [0, 1)$: Exponential decay rates
\REQUIRE $\boldsymbol{\theta}_0$: Initial parameter vector
\REQUIRE $\mathbf{m}_0 \leftarrow \mathbf{0}, \mathbf{v}_0 \leftarrow \mathbf{0}$
\STATE $t \leftarrow 0$
\WHILE{$\boldsymbol{\theta}_t$ not converged}
    \STATE $t \leftarrow t + 1$
    \STATE Get gradients $\mathbf{g}_t \leftarrow \nabla_{\boldsymbol{\theta}} f_t(\boldsymbol{\theta}_{t-1})$
    \IF{$t > 1$}
        \STATE Compute noise variance: $\hat{\sigma}^2 \leftarrow \text{mean}(\mathbf{v}_{t-1} - \mathbf{m}_{t-1}^2)$
        \STATE Compute divergence: $D_n \leftarrow || \mathbf{g}_t - \mathbf{m}_{t-1} ||^2$
        \STATE Compute shrinkage factor: $c_t \leftarrow \max\left(0, 1 - \frac{(p-2)\hat{\sigma}^2}{D_n}\right)$
        \STATE \textbf{Stein Correction}: $\hat{\mathbf{g}}_t \leftarrow \mathbf{m}_{t-1} + c_t (\mathbf{g}_t - \mathbf{m}_{t-1})$
    \ELSE
        \STATE $\hat{\mathbf{g}}_t \leftarrow \mathbf{g}_t$
    \ENDIF
    \STATE Update biased first moment: $\mathbf{m}_t \leftarrow \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \hat{\mathbf{g}}_t$
    \STATE Update biased second raw moment: $\mathbf{v}_t \leftarrow \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) \hat{\mathbf{g}}_t^2$
    \STATE Compute bias-corrected moments $\hat{\mathbf{m}}_t, \hat{\mathbf{v}}_t$
    \STATE Update parameters: $\boldsymbol{\theta}_t \leftarrow \boldsymbol{\theta}_{t-1} - \alpha \cdot \hat{\mathbf{m}}_t / (\sqrt{\hat{\mathbf{v}}_t} + \epsilon)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
\section{Discussion and Conclusion}
The SR-Adam algorithm introduces a rigorous statistical "gate" to the optimization process. In regimes of high uncertainty (early training or noisy batches), the term $\frac{(p-2)\sigma^2}{D_n}$ dominates, and $c_t \to 0$. The optimizer effectively rejects the noisy observation $\mathbf{g}_t$ and defaults to the stable momentum $\mathbf{m}_{t-1}$. As training stabilizes and variance decreases, $c_t \to 1$, recovering the standard Adam behavior.
This methodology demonstrates that the inadmissibility of high-dimensional estimators is not merely a theoretical curiosity but a practical lever for improving deep learning optimization. Future work will extend this to Preliminary Test (PT) strategies for dynamic network pruning.
\begin{thebibliography}{1}
\bibitem{stein1956} C. Stein, "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution," \textit{Proc. Third Berkeley Symp. Math. Statist. Prob.}, vol. 1, pp. 197-206, 1956.
\bibitem{james1961} W. James and C. Stein, "Estimation with quadratic loss," \textit{Proc. Fourth Berkeley Symp. Math. Statist. Prob.}, vol. 1, pp. 361-379, 1961.
\bibitem{saleh2006} A.K. Md. E. Saleh, \textit{Theory of Preliminary Test and Stein-Type Estimation with Applications}, Wiley, 2006.
\end{thebibliography}
\end{document}
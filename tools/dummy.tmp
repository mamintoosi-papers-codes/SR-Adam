"""
Generate LaTeX table with SimpleCNN architecture and parameter count.
"""


def count_parameters_simplecnn():
    """Calculate SimpleCNN total parameters."""
    # Conv1: 3 input channels × 32 output channels × 3×3 kernel + 32 bias = 896
    conv1 = 3 * 32 * 3 * 3 + 32
    # Conv2: 32 input channels × 64 output channels × 3×3 kernel + 64 bias = 18496
    conv2 = 32 * 64 * 3 * 3 + 64
    # FC1: 64×8×8 = 4096 inputs × 128 outputs + 128 bias = 524416
    fc1 = 4096 * 128 + 128
    # FC2: 128 inputs × 10 outputs + 10 bias = 1290
    fc2 = 128 * 10 + 10
    
    total = conv1 + conv2 + fc1 + fc2
    return total


def generate_simplecnn_table():
    """Generate LaTeX table for SimpleCNN architecture."""
    total_params = count_parameters_simplecnn()
    lines = []
    
    lines.append("% SimpleCNN Architecture")
    lines.append("\\begin{table}[t]")
    lines.append("  \\centering")
    lines.append("  \\caption{SimpleCNN architecture: layer configuration, output shape, and parameters.}")
    lines.append("  \\label{tab:simplecnn_arch}")
    lines.append("  \\begin{tabular}{l c c c}")
    lines.append("    \\toprule")
    lines.append("    Layer & Kernel/Units & Output Shape & Parameters \\\\")
    lines.append("    \\midrule")
    
    # Layer details
    layers = [
        ("Conv2d-1", "3 → 32, 3×3", "32×32×32", "32×3×3×3 + 32 = 896"),
        ("ReLU-1", "—", "32×32×32", "0"),
        ("MaxPool2d-1", "2×2, stride=2", "32×16×16", "0"),
        ("Conv2d-2", "32 → 64, 3×3", "64×16×16", "64×32×3×3 + 64 = 18,496"),
        ("ReLU-2", "—", "64×16×16", "0"),
        ("MaxPool2d-2", "2×2, stride=2", "64×8×8", "0"),
        ("Flatten", "—", "4,096", "0"),
        ("Linear-1", "4,096 → 128", "128", "4,096×128 + 128 = 524,416"),
        ("ReLU-3", "—", "128", "0"),
        ("Dropout (0.2)", "—", "128", "0"),
        ("Linear-2", "128 → 10", "10", "128×10 + 10 = 1,290"),
    ]
    
    for layer, config, output, params in layers:
        lines.append(f"    {layer} & {config} & {output} & {params} \\\\")
    
    lines.append("    \\midrule")
    lines.append(f"    \\textbf{{Total}} & — & — & \\textbf{{{total_params:,}}} \\\\")
    lines.append("    \\bottomrule")
    lines.append("  \\end{tabular}")
    lines.append("\\end{table}")
    lines.append("")
    
    return "\n".join(lines)


def generate_architecture_content():
    """Generate body-only content with architecture tables."""
    total_params = count_parameters_simplecnn()
    lines = []
    
    lines.append("% SimpleCNN Architecture")
    lines.append("\\begin{table}[t]")
    lines.append("  \\centering")
    lines.append("  \\caption{SimpleCNN architecture: layer configuration, output shape, and parameters.}")
    lines.append("  \\label{tab:simplecnn_arch}")
    lines.append("  \\begin{tabular}{l c c c}")
    lines.append("    \\toprule")
    lines.append("    Layer & Kernel/Units & Output Shape & Parameters \\\\")
    lines.append("    \\midrule")
    
    layers = [
        ("Conv2d-1", "3 → 32, 3×3", "32×32×32", "896"),
        ("ReLU-1", "—", "32×32×32", "0"),
        ("MaxPool2d-1", "2×2, stride=2", "32×16×16", "0"),
        ("Conv2d-2", "32 → 64, 3×3", "64×16×16", "18,496"),
        ("ReLU-2", "—", "64×16×16", "0"),
        ("MaxPool2d-2", "2×2, stride=2", "64×8×8", "0"),
        ("Flatten", "—", "4,096", "0"),
        ("Linear-1", "4,096 → 128", "128", "524,416"),
        ("ReLU-3", "—", "128", "0"),
        ("Dropout (0.2)", "—", "128", "0"),
        ("Linear-2", "128 → 10", "10", "1,290"),
    ]
    
    for layer, config, output, params in layers:
        lines.append(f"    {layer} & {config} & {output} & {params} \\\\")
    
    lines.append("    \\midrule")
    lines.append(f"    \\textbf{{Total}} & — & — & \\textbf{{{total_params:,}}} \\\\")
    lines.append("    \\bottomrule")
    lines.append("  \\end{tabular}")
    lines.append("\\end{table}")
    lines.append("")
    
    return "\n".join(lines)


if __name__ == '__main__':
    import os
    out_dir = os.path.join('paper')
    os.makedirs(out_dir, exist_ok=True)
    
    # Write standalone table
    content = generate_simplecnn_table()
    out_path = os.path.join(out_dir, 'simplecnn_arch.tex')
    with open(out_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"Wrote {out_path}")
    
    # Write body-only content
    body_content = generate_architecture_content()
    body_out_path = os.path.join(out_dir, 'simplecnn_arch_content.tex')
    with open(body_out_path, 'w', encoding='utf-8') as f:
        f.write(body_content)
    print(f"Wrote {body_out_path}")

"""
Generate qualitative comparison figure for paper:
Show sample CIFAR images with Adam vs SR-Adam predictions at noise level 0.05
"""

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import os
from pathlib import Path
import json
import argparse

# Import model architecture
from src.model import get_model


def load_checkpoint(checkpoint_path, model, device='cpu'):
    """Load model from checkpoint."""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    return model, checkpoint.get('test_acc', None)


def get_best_run(optimizer_name, dataset='CIFAR10', model_name='simplecnn', noise=0.05, runs_root='runs'):
    """Find the run with highest best accuracy for given optimizer."""
    folder = Path(runs_root) / dataset / model_name / f'noise_{noise}' / optimizer_name
    
    if not folder.exists():
        folder = Path('results') / dataset / model_name / f'noise_{noise}' / optimizer_name
    
    if not folder.exists():
        raise FileNotFoundError(f"No results found for {optimizer_name} in {folder}")
    
    # Find all best checkpoints
    best_files = list(folder.glob('run_*_best.pt'))
    
    if not best_files:
        raise FileNotFoundError(f"No best checkpoints found in {folder}")
    
    # Load all and find highest accuracy
    best_acc = -1
    best_path = None
    
    for f in best_files:
        try:
            ckpt = torch.load(f, map_location='cpu')
            acc = ckpt.get('test_acc', -1)
            if acc > best_acc:
                best_acc = acc
                best_path = f
        except:
            continue
    
    return best_path, best_acc


def get_cifar_classes(dataset_name):
    """Return class names for CIFAR datasets."""
    if dataset_name == 'CIFAR10':
        return ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                'dog', 'frog', 'horse', 'ship', 'truck']
    else:  # CIFAR100
        return [
            'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',
            'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',
            'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',
            'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',
            'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',
            'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
            'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
            'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',
            'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
            'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
            'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
            'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
        ]


def generate_comparison_table(dataset='CIFAR10', noise=0.05, num_samples=10, 
                               save_path='paper/qualitative_comparison.pdf',
                               runs_root='runs', random_seed=None):
    """
    Generate comparison table showing sample images with Adam vs SR-Adam predictions.
    
    Args:
        dataset: 'CIFAR10' or 'CIFAR100'
        noise: noise level (0.0, 0.05, 0.1)
        num_samples: number of images to show
        save_path: where to save the figure
        runs_root: root directory for model checkpoints
        random_seed: seed for sample selection (None = random each time)
    """
    # Set random seed for reproducible sample selection
    if random_seed is not None:
        np.random.seed(random_seed)
        print(f"Using random seed: {random_seed}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load class names
    class_names = get_cifar_classes(dataset)
    num_classes = len(class_names)
    
    # Load test dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    if dataset == 'CIFAR10':
        testset = torchvision.datasets.CIFAR10(root='./data', train=False, 
                                                download=True, transform=transform)
    else:
        testset = torchvision.datasets.CIFAR100(root='./data', train=False,
                                                 download=True, transform=transform)
    
    # Find best checkpoints for both optimizers
    print("Loading best models...")
    adam_path, adam_acc = get_best_run('Adam', dataset, 'simplecnn', noise, runs_root)
    sradam_path, sradam_acc = get_best_run('SR-Adam', dataset, 'simplecnn', noise, runs_root)
    
    print(f"\n{'='*60}")
    print(f"LOADED CHECKPOINT ACCURACIES (from best.pt files):")
    print(f"{'='*60}")
    print(f"Adam best checkpoint:    {adam_acc:.2f}%")
    print(f"SR-Adam best checkpoint: {sradam_acc:.2f}%")
    print(f"{'='*60}")
    print(f"Adam best: {adam_path}")
    print(f"SR-Adam best: {sradam_path}")
    print(f"{'='*60}\n")
    
    # Load models
    adam_model = get_model('simplecnn', num_classes=num_classes).to(device)
    sradam_model = get_model('simplecnn', num_classes=num_classes).to(device)
    
    adam_model, _ = load_checkpoint(adam_path, adam_model, device)
    sradam_model, _ = load_checkpoint(sradam_path, sradam_model, device)
    
    # Select diverse samples (try to get variety of classes)
    indices = []
    classes_seen = set()
    
    for idx in np.random.permutation(len(testset)):
        _, label = testset[idx]
        if label not in classes_seen or len(indices) < num_samples:
            indices.append(idx)
            classes_seen.add(label)
        if len(indices) >= num_samples:
            break
    
    # Create figure: TRANSPOSED layout (3 rows × num_samples columns)
    # Rows: True Label, Adam, SR-Adam
    # Columns: Sample images
    fig, axes = plt.subplots(3, num_samples, figsize=(2*num_samples, 7))
    
    if num_samples == 1:
        axes = axes.reshape(-1, 1)
    
    print(f"\nGenerating predictions for {num_samples} samples...")
    
    for col, idx in enumerate(indices):
        image, true_label = testset[idx]
        
        # Predict with both models
        with torch.no_grad():
            img_batch = image.unsqueeze(0).to(device)
            
            adam_out = adam_model(img_batch)
            adam_pred = adam_out.argmax(dim=1).item()
            adam_conf = torch.softmax(adam_out, dim=1).max().item()
            
            sradam_out = sradam_model(img_batch)
            sradam_pred = sradam_out.argmax(dim=1).item()
            sradam_conf = torch.softmax(sradam_out, dim=1).max().item()
        
        # Denormalize image for display
        img_np = image.numpy().transpose(1, 2, 0)
        img_np = img_np * 0.5 + 0.5  # denormalize
        img_np = np.clip(img_np, 0, 1)
        
        # Row 0: Ground Truth
        axes[0, col].imshow(img_np)
        axes[0, col].set_title(f'{class_names[true_label]}', fontsize=10, fontweight='bold')
        axes[0, col].axis('off')
        
        # Row 1: Adam prediction
        adam_color = 'green' if adam_pred == true_label else 'red'
        axes[1, col].imshow(img_np)
        axes[1, col].set_title(f'{class_names[adam_pred]}\n({adam_conf:.2f})', 
                               color=adam_color, fontsize=9, fontweight='bold')
        axes[1, col].axis('off')
        
        # Row 2: SR-Adam prediction
        sradam_color = 'green' if sradam_pred == true_label else 'red'
        axes[2, col].imshow(img_np)
        axes[2, col].set_title(f'{class_names[sradam_pred]}\n({sradam_conf:.2f})', 
                               color=sradam_color, fontsize=9, fontweight='bold')
        axes[2, col].axis('off')
    
    # Add row labels on the left
    fig.text(0.02, 0.75, 'Ground\nTruth', ha='center', va='center', fontsize=12, fontweight='bold', rotation=0)
    fig.text(0.02, 0.50, f'Adam\n({adam_acc:.1f}%)', ha='center', va='center', fontsize=12, fontweight='bold', rotation=0)
    fig.text(0.02, 0.25, f'SR-Adam\n({sradam_acc:.1f}%)', ha='center', va='center', fontsize=12, fontweight='bold', rotation=0)
    
    plt.tight_layout(rect=[0.05, 0, 1, 1])
    
    # Save
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\n✓ Saved comparison figure to {save_path}")
    
    plt.show()


if __name__ == "__main__":
    import json
    
    parser = argparse.ArgumentParser(description='Generate qualitative comparison figures for paper')
    parser.add_argument('--dataset', type=str, default='CIFAR10', choices=['CIFAR10', 'CIFAR100'],
                        help='Dataset name (default: CIFAR10)')
    parser.add_argument('--noise', type=float, default=0.05, choices=[0.0, 0.05, 0.1],
                        help='Noise level (default: 0.05)')
    parser.add_argument('--seed', type=int, default=None,
                        help='Random seed for sample selection (default: None=random). Try different seeds like 42, 123, 999 to find best visualization')
    parser.add_argument('--num-samples', type=int, default=10,
                        help='Number of samples to display (default: 10)')
    parser.add_argument('--runs-root', type=str, default='runs',
                        help='Root directory for model checkpoints (default: runs)')
    
    args = parser.parse_args()
    
    dataset = args.dataset
    noise = args.noise
    seed = args.seed
    
    # Print aggregated summary values for verification
    print("\n" + "="*70)
    print(f"AGGREGATED SUMMARY (from results/.../aggregated_summary.json):")
    print(f"Dataset: {dataset}, Noise: {noise}")
    if seed is not None:
        print(f"Random Seed: {seed}")
    print("="*70)
    
    summary_data = {}
    for opt in ['Adam', 'SR-Adam']:
        json_path = f'results/{dataset}/simplecnn/noise_{noise}/{opt}/aggregated_summary.json'
        if os.path.exists(json_path):
            with open(json_path, 'r') as f:
                data = json.load(f)
            print(f"\n{opt}:")
            print(f"  Final: {data['final_test_acc_mean']:.2f} ± {data['final_test_acc_std']:.2f}%")
            print(f"  Best:  {data['best_test_acc_mean']:.2f} ± {data['best_test_acc_std']:.2f}%")
            summary_data[opt] = data
    print("="*70 + "\n")
    
    # Generate comparison
    dataset_lower = dataset.lower()
    noise_str = str(noise).replace('.', '')
    save_path = f'paper/qualitative_{dataset_lower}_noise{noise_str}.pdf'
    
    print(f"Generating qualitative comparison figure for {dataset} with noise={noise}...")
    if seed is not None:
        print(f"TIP: Try different seeds (e.g., --seed 42, --seed 123, --seed 999) to find the most informative samples!")
    
    generate_comparison_table(
        dataset=dataset,
        noise=noise,
        num_samples=args.num_samples,
        save_path=save_path,
        runs_root=args.runs_root,
        random_seed=seed
    )
    
    # After generation, load the checkpoint accuracies that were used
    print("\n" + "="*70)
    print("CHECKPOINT ACCURACIES USED IN PDF (from best.pt files):")
    print("="*70)
    
    checkpoint_accs = {}
    for opt in ['Adam', 'SR-Adam']:
        try:
            _, acc = get_best_run(opt, dataset, 'simplecnn', noise, args.runs_root)
            checkpoint_accs[opt] = acc
            print(f"{opt}: {acc:.2f}%")
        except Exception as e:
            print(f"{opt}: Error - {e}")
    
    print("="*70)
    
    # Save comparison to JSON
    comparison = {
        "dataset": dataset,
        "noise": noise,
        "random_seed": seed,
        "aggregated_summary": summary_data,
        "checkpoint_best_selected": checkpoint_accs,
        "note": "checkpoint_best_selected shows the single best run accuracy displayed in PDF"
    }
    
    os.makedirs('paper', exist_ok=True)
    json_name = f'qualitative_accuracy_comparison_{dataset_lower}_noise{noise_str}.json'
    with open(f'paper/{json_name}', 'w') as f:
        json.dump(comparison, f, indent=2)
    
    print(f"\n✓ Saved comparison to paper/{json_name}")
    print(f"\n✓ Figure saved to {save_path}")
    print("\nKEY INSIGHT:")
    print("  - Table shows MEAN ± STD across 5 runs")
    print("  - PDF shows accuracy of the BEST SINGLE RUN (highest checkpoint)")
    print("  - This explains why PDF values differ from table values!")
    if seed is not None:
        print(f"\nUsed seed={seed}. To try different samples, use --seed with another value.")
    print("="*70 + "\n")

import os
import glob
import pandas as pd
import matplotlib.pyplot as plt

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), "results")
FIG_DIR = os.path.join(os.path.dirname(__file__), "paper")
os.makedirs(FIG_DIR, exist_ok=True)

plt.style.use('seaborn-v0_8')


def load_epoch_stats(dataset, model, noise, optimizer):
    path = os.path.join(RESULTS_ROOT, dataset, model, f"noise_{noise}", optimizer, "aggregated_epoch_stats.csv")
    if not os.path.isfile(path):
        raise FileNotFoundError(f"missing aggregated_epoch_stats: {path}")
    return pd.read_csv(path)


def figure_epoch_curve():
    dataset = "CIFAR10"
    model = "simplecnn"
    noise = "0.1"
    optimizers = ["Adam", "SR-Adam"]

    fig, ax = plt.subplots(figsize=(5, 3.2))
    for opt in optimizers:
        df = load_epoch_stats(dataset, model, noise, opt)
        ax.plot(df["Epoch"], df["Test Acc Mean"], label=opt, linewidth=2)
        ax.fill_between(df["Epoch"], df["Test Acc Mean"] - df["Test Acc Std"], df["Test Acc Mean"] + df["Test Acc Std"], alpha=0.2)

    ax.set_xlabel("Epoch")
    ax.set_ylabel("Test Accuracy (%)")
    ax.set_title(f"{dataset} (noise={noise})")
    ax.legend()
    fig.tight_layout()
    out_path = os.path.join(FIG_DIR, "cifar10_noise0p1_epoch.pdf")
    fig.savefig(out_path)
    plt.close(fig)
    return out_path


def load_summary():
    df = pd.read_csv(os.path.join(RESULTS_ROOT, "summary_statistics.csv"), index_col=0)
    df = df.reset_index().rename(columns={"index": "key"})
    parts = df["key"].str.split("|", expand=True)
    df["dataset"] = parts[0]
    df["noise"] = parts[1].str.replace("noise_", "", regex=False)
    df["optimizer"] = parts[2]
    return df


def figure_noise_sweep():
    df = load_summary()
    datasets = [
        ("CIFAR10", "simplecnn"),
        ("CIFAR100", "resnet18"),
    ]
    optimizers = ["Adam", "SR-Adam"]
    outputs = []

    for dataset, _model in datasets:
        fig, ax = plt.subplots(figsize=(5, 3.2))
        for opt in optimizers:
            sub = df[(df["dataset"] == dataset) & (df["optimizer"] == opt)]
            sub = sub[sub["noise"].isin(["0.0", "0.05", "0.1"])]
            sub = sub.sort_values("noise", key=lambda s: s.astype(float))
            ax.errorbar(sub["noise"].astype(float), sub["final_mean"], yerr=sub["final_std"], label=opt, linewidth=2, capsize=4, marker="o")
        ax.set_xlabel("Noise Std")
        ax.set_ylabel("Final Test Accuracy (%)")
        ax.set_title(dataset)
        ax.legend()
        fig.tight_layout()
        out_path = os.path.join(FIG_DIR, f"{dataset.lower()}_noise_sweep.pdf")
        fig.savefig(out_path)
        plt.close(fig)
        outputs.append(out_path)

    return outputs


def main():
    epoch_fig = figure_epoch_curve()
    noise_figs = figure_noise_sweep()
    print("Saved figures:")
    print(epoch_fig)
    for f in noise_figs:
        print(f)


if __name__ == "__main__":
    main()

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), "results")
FIG_DIR = os.path.join(os.path.dirname(__file__), "paper")
os.makedirs(FIG_DIR, exist_ok=True)

OPT_ORDER = ["SGD", "Momentum", "Adam", "SR-Adam"]
COLORS = None  # use matplotlib default cycle; order controlled by OPT_ORDER


def load_runs(opt_path):
    run_files = sorted(glob.glob(os.path.join(opt_path, "run_*.csv")))
    if not run_files:
        return None
    runs = [pd.read_csv(f) for f in run_files]
    max_len = max(len(r) for r in runs)
    def pad_col(col):
        mat = np.zeros((len(runs), max_len))
        for i, r in enumerate(runs):
            arr = r[col].values
            mat[i, : len(arr)] = arr
        return mat
    train_loss = pad_col("Train Loss")
    test_loss = pad_col("Test Loss")
    epochs = np.arange(1, max_len + 1)
    return epochs, train_loss, test_loss


def plot_loss(dataset, noise, model="simplecnn"):
    noise_dir = f"noise_{noise}"
    base = os.path.join(RESULTS_ROOT, dataset, model, noise_dir)
    if not os.path.isdir(base):
        return None

    fig, ax = plt.subplots(figsize=(6, 4))
    for idx, opt in enumerate(OPT_ORDER):
        opt_path = os.path.join(base, opt)
        data = load_runs(opt_path)
        if data is None:
            continue
        epochs, train_loss, test_loss = data
        mean = test_loss.mean(axis=0)
        std = test_loss.std(axis=0)
        ax.plot(epochs, mean, label=opt, linewidth=2)
        ax.fill_between(epochs, mean - std, mean + std, alpha=0.2)

    ax.set_xlabel("Epoch")
    ax.set_ylabel("Test Loss")
    ax.set_title(f"{dataset} / {model} (noise={noise})")
    ax.legend()
    ax.grid(True)
    fig.tight_layout()

    out_path = os.path.join(FIG_DIR, f"{dataset.lower()}_noise{noise}_loss_mean_std.pdf")
    fig.savefig(out_path)
    plt.close(fig)
    return out_path


def main():
    outputs = []
    for dataset in ["CIFAR10", "CIFAR100"]:
        for noise in ["0.0", "0.05", "0.1"]:
            out = plot_loss(dataset, noise)
            if out:
                outputs.append(out)
    if outputs:
        print("Saved loss figures:")
        for o in outputs:
            print(o)
    else:
        print("No figures generated (missing runs?)")


if __name__ == "__main__":
    main()

import os
import glob
import pandas as pd
import numpy as np

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), 'results')
OUT_DIR = os.path.join(os.path.dirname(__file__), 'paper')
os.makedirs(OUT_DIR, exist_ok=True)

METHOD_ORDER = ["SGD", "Momentum", "Adam", "SR-Adam"]
NOISES = ["0.0", "0.05", "0.1"]
DATASETS = ["CIFAR10", "CIFAR100"]
MODEL = "simplecnn"


def collect_runs(dataset, noise, optimizer):
    folder = os.path.join(RESULTS_ROOT, dataset, MODEL, f"noise_{noise}", optimizer)
    files = sorted(glob.glob(os.path.join(folder, 'run_*.csv')))
    return [pd.read_csv(f) for f in files]


def compute_stats(runs):
    # Each run is a DF with columns: Train Loss, Test Loss, Train Acc, Test Acc
    finals_acc = [float(r['Test Acc'].iloc[-1]) for r in runs]
    bests_acc = [float(r['Test Acc'].max()) for r in runs]
    finals_loss = [float(r['Test Loss'].iloc[-1]) for r in runs]
    bests_loss = [float(r['Test Loss'].min()) for r in runs]  # lower is better
    return {
        'final_acc_mean': float(np.mean(finals_acc)),
        'final_acc_std': float(np.std(finals_acc, ddof=0)),
        'best_acc_mean': float(np.mean(bests_acc)),
        'best_acc_std': float(np.std(bests_acc, ddof=0)),
        'final_loss_mean': float(np.mean(finals_loss)),
        'final_loss_std': float(np.std(finals_loss, ddof=0)),
        'best_loss_mean': float(np.mean(bests_loss)),
        'best_loss_std': float(np.std(bests_loss, ddof=0)),
    }


def format_cell(mean, std, bold=False):
    cell = f"{mean:.2f} \\pm {std:.2f}"
    return f"\\textbf{{{cell}}}" if bold else cell


def build_table(metric_key, metric_label, higher_is_better, out_tex):
    # columns: CIFAR10 (0.0,0.05,0.1) | CIFAR100 (0.0,0.05,0.1)
    lines = []
    lines.append("\\documentclass[varwidth=\\maxdimen]{standalone}")
    lines.append("\\usepackage{booktabs}")
    lines.append("\\begin{document}")
    lines.append("\\begin{tabular}{l ccc ccc}")
    lines.append("\\toprule")
    lines.append(" & \\multicolumn{3}{c}{CIFAR10} & \\multicolumn{3}{c}{CIFAR100} \\")
    lines.append("Method & 0.0 & 0.05 & 0.1 & 0.0 & 0.05 & 0.1 \\\\")
    lines.append("\\midrule")

    for method in METHOD_ORDER:
        row_cells = [method]
        values = {}
        # Compute per dataset/noise
        for dataset in DATASETS:
            for noise in NOISES:
                runs = collect_runs(dataset, noise, method)
                if not runs:
                    values[(dataset, noise)] = (float('nan'), float('nan'))
                    continue
                stats = compute_stats(runs)
                mean = stats[f'{metric_key}_mean']
                std = stats[f'{metric_key}_std']
                values[(dataset, noise)] = (mean, std)
        # Determine best per column
        for dataset in DATASETS:
            for noise in NOISES:
                col_vals = {m: values[(dataset, noise)][0] for m in METHOD_ORDER}
                if higher_is_better:
                    best_method = max(col_vals, key=lambda k: col_vals[k])
                else:
                    best_method = min(col_vals, key=lambda k: col_vals[k])
                mean, std = values[(dataset, noise)]
                row_cells.append(format_cell(mean, std, bold=(method == best_method)))
        lines.append(" ".join(row_cells) + " \\\\")

    lines.append("\\bottomrule")
    lines.append("\\end{tabular}")
    lines.append(f"\\end{{document}}")

    with open(os.path.join(OUT_DIR, out_tex), 'w', encoding='utf-8') as f:
        f.write("\n".join(lines))
    print(f"Wrote {out_tex}")


def main():
    # Two tables: best accuracy (higher better) and best loss (lower better)
    build_table('best_acc', 'Best Accuracy', True, 'methods_best_acc.tex')
    build_table('best_loss', 'Best Loss', False, 'methods_best_loss.tex')
    # Optionally, also final
    build_table('final_acc', 'Final Accuracy', True, 'methods_final_acc.tex')
    build_table('final_loss', 'Final Loss', False, 'methods_final_loss.tex')


if __name__ == '__main__':
    main()

import os
import glob
import pandas as pd
import numpy as np

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), 'results')
OUT_DIR = os.path.join(os.path.dirname(__file__), 'paper')
os.makedirs(OUT_DIR, exist_ok=True)

METHOD_ORDER = ["SGD", "Momentum", "Adam", "SR-Adam"]
NOISES = ["0.0", "0.05", "0.1"]
DATASETS = ["CIFAR10", "CIFAR100"]
MODEL = "simplecnn"


def collect_runs(dataset, noise, optimizer):
    folder = os.path.join(RESULTS_ROOT, dataset, MODEL, f"noise_{noise}", optimizer)
    files = sorted(glob.glob(os.path.join(folder, 'run_*.csv')))
    return [pd.read_csv(f) for f in files]


def compute_stats(runs):
    finals_acc = [float(r['Test Acc'].iloc[-1]) for r in runs]
    bests_acc = [float(r['Test Acc'].max()) for r in runs]
    finals_loss = [float(r['Test Loss'].iloc[-1]) for r in runs]
    bests_loss = [float(r['Test Loss'].min()) for r in runs]
    return {
        'final_acc_mean': float(np.mean(finals_acc)),
        'final_acc_std': float(np.std(finals_acc, ddof=1)),
        'best_acc_mean': float(np.mean(bests_acc)),
        'best_acc_std': float(np.std(bests_acc, ddof=1)),
        'final_loss_mean': float(np.mean(finals_loss)),
        'final_loss_std': float(np.std(finals_loss, ddof=1)),
        'best_loss_mean': float(np.mean(bests_loss)),
        'best_loss_std': float(np.std(bests_loss, ddof=1)),
    }


def format_cell(mean, std, bold=False):
    if np.isnan(mean) or np.isnan(std):
        return "--"
    cell = f"{mean:.2f} $\\pm$ {std:.2f}"
    return f"\\textbf{{{cell}}}" if bold else cell


def gather_values(metric_key, higher_is_better):
    # Build a dict: method -> {(dataset, noise): (mean,std)} and best markers
    values = {m: {} for m in METHOD_ORDER}
    for method in METHOD_ORDER:
        for dataset in DATASETS:
            for noise in NOISES:
                runs = collect_runs(dataset, noise, method)
                if not runs:
                    values[method][(dataset, noise)] = (float('nan'), float('nan'))
                    continue
                stats = compute_stats(runs)
                mean = stats[f'{metric_key}_mean']
                std = stats[f'{metric_key}_std']
                values[method][(dataset, noise)] = (mean, std)
    # Determine best per column
    best_flags = {m: {} for m in METHOD_ORDER}
    for dataset in DATASETS:
        for noise in NOISES:
            col_vals = {m: values[m][(dataset, noise)][0] for m in METHOD_ORDER}
            if higher_is_better:
                best_method = max(col_vals, key=lambda k: col_vals[k])
            else:
                best_method = min(col_vals, key=lambda k: col_vals[k])
            for m in METHOD_ORDER:
                best_flags[m][(dataset, noise)] = (m == best_method)
    return values, best_flags


def build_table_tex(metric_title, values, best_flags, label=None):
    lines = []
    lines.append("% " + metric_title)
    lines.append("\\begin{table}[t]")
    lines.append("  \\centering")
    lines.append("  \\begin{tabular}{l ccc ccc}")
    lines.append("    \\toprule")
    lines.append("     & \\multicolumn{3}{c}{CIFAR10} & \\multicolumn{3}{c}{CIFAR100} \\\\")
    lines.append("     \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}")
    lines.append("    Method & 0.0 & 0.05 & 0.1 & 0.0 & 0.05 & 0.1 \\\\")
    lines.append("    \\midrule")
    for method in METHOD_ORDER:
        row = [method]
        for dataset in DATASETS:
            for noise in NOISES:
                mean, std = values[method][(dataset, noise)]
                bold = best_flags[method][(dataset, noise)]
                row.append(format_cell(mean, std, bold=bold))
        lines.append("    " + " & ".join(row) + " \\\\")
    lines.append("    \\bottomrule")
    lines.append("  \\end{tabular}")
    lines.append(f"  \\caption{{{metric_title}}}")
    if label:
        lines.append(f"  \\label{{{label}}}")
    lines.append("\\end{table}")
    lines.append("")
    return "\n".join(lines)


def main():
    parts = []
    body_parts = []

    # Best Accuracy (higher better)
    vals, flags = gather_values('best_acc', True)
    body_parts.append(build_table_tex('Best test accuracy (mean $\\pm$ std) over epochs; higher is better.', vals, flags, label='tab:method_best_acc'))

    # Final Accuracy (higher better)
    vals, flags = gather_values('final_acc', True)
    body_parts.append(build_table_tex('Final test accuracy (mean $\\pm$ std) at last epoch; higher is better.', vals, flags, label='tab:method_final_acc'))

    # Best Loss (lower better)
    vals, flags = gather_values('best_loss', False)
    body_parts.append(build_table_tex('Best test loss (mean $\\pm$ std) over epochs; lower is better.', vals, flags, label='tab:method_best_loss'))

    # Final Loss (lower better)
    vals, flags = gather_values('final_loss', False)
    body_parts.append(build_table_tex('Final test loss (mean $\\pm$ std) at last epoch; lower is better.', vals, flags, label='tab:method_final_loss'))
    # Write full standalone doc
    parts.append("\\documentclass{article}")
    parts.append("\\usepackage{booktabs}")
    parts.append("\\usepackage[margin=1in]{geometry}")
    parts.append("\\begin{document}")
    parts.append("\n".join(body_parts))
    parts.append("\\end{document}")
    out_path = os.path.join(OUT_DIR, 'minimal-tables.tex')
    with open(out_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(parts))
    print(f"Wrote {out_path}")

    # Write body-only content for \input into the main paper
    body_out_path = os.path.join(OUT_DIR, 'minimal-tables-content.tex')
    with open(body_out_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(body_parts))
    print(f"Wrote {body_out_path}")


if __name__ == '__main__':
    main()

import os
import pandas as pd

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), "results")
SUMMARY_CSV = os.path.join(RESULTS_ROOT, "summary_statistics.csv")
TABLES_TEX = os.path.join(os.path.dirname(__file__), "tables.tex")

NOISE_LEVELS = ["0.0", "0.05", "0.1"]
OPTIMIZERS = ["Adam", "SR-Adam"]


def _load_summary():
    df = pd.read_csv(SUMMARY_CSV, index_col=0)
    df = df.reset_index().rename(columns={"index": "key"})
    parts = df["key"].str.split("|", expand=True)
    df["dataset"] = parts[0]
    df["noise"] = parts[1].str.replace("noise_", "", regex=False)
    df["optimizer"] = parts[2]
    return df


def _format_row(mean, std, is_best):
    cell = f"{mean:.2f} \\pm {std:.2f}"
    return f"\\textbf{{{cell}}}" if is_best else cell


def build_table(df, dataset, mean_col, std_col, caption, label):
    subset = df[(df["dataset"] == dataset) & (df["optimizer"].isin(OPTIMIZERS)) & (df["noise"].isin(NOISE_LEVELS))]

    lines = []
    lines.append("\\begin{table}[t]")
    lines.append("  \\centering")
    lines.append("  \\begin{tabular}{lcc}")
    lines.append("    \\toprule")
    lines.append("    Noise & Adam & SR-Adam \\\\")
    lines.append("    \\midrule")

    for noise in NOISE_LEVELS:
        row = subset[subset["noise"] == noise].set_index("optimizer")
        vals = {}
        for opt in OPTIMIZERS:
            if opt not in row.index:
                vals[opt] = (float("nan"), float("nan"))
            else:
                vals[opt] = (row.loc[opt, mean_col], row.loc[opt, std_col])
        best_opt = max(OPTIMIZERS, key=lambda o: vals[o][0])
        adam_cell = _format_row(*vals["Adam"], is_best=(best_opt == "Adam"))
        sr_cell = _format_row(*vals["SR-Adam"], is_best=(best_opt == "SR-Adam"))
        lines.append(f"    {noise} & {adam_cell} & {sr_cell} \\")

    lines.append("    \\bottomrule")
    lines.append("  \\end{tabular}")
    lines.append(f"  \\caption{{{caption}}}")
    lines.append(f"  \\label{{{label}}}")
    lines.append("\\end{table}")
    lines.append("")
    return "\n".join(lines)


def main():
    df = _load_summary()

    table1 = []
    table2 = []
    for dataset in ["CIFAR10", "CIFAR100"]:
        table1.append(
            build_table(
                df,
                dataset,
                mean_col="final_mean",
                std_col="final_std",
                caption=f"Final test accuracy (mean $\\pm$ std) on {dataset}.",
                label=f"tab:{dataset.lower()}-final",
            )
        )
        table2.append(
            build_table(
                df,
                dataset,
                mean_col="best_mean",
                std_col="best_std",
                caption=f"Best test accuracy over epochs (mean $\\pm$ std) on {dataset}.",
                label=f"tab:{dataset.lower()}-best",
            )
        )

    content = [
        "% Auto-generated; do not edit by hand",
        "\\usepackage{booktabs}",
        "",
        "% Table 1: Final accuracy",
        *table1,
        "% Table 2: Best accuracy",
        *table2,
    ]

    with open(TABLES_TEX, "w", encoding="utf-8") as f:
        f.write("\n".join(content))
    print(f"Wrote LaTeX tables to {TABLES_TEX}")


if __name__ == "__main__":
    main()

import os
import glob
import pandas as pd
import matplotlib.pyplot as plt

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), "results")
FIG_DIR = os.path.join(os.path.dirname(__file__), "paper")
os.makedirs(FIG_DIR, exist_ok=True)

OPT_ORDER = ["SGD", "Momentum", "Adam", "SR-Adam"]


def load_agg(opt_path):
    agg_path = os.path.join(opt_path, "aggregated_epoch_stats.csv")
    if not os.path.isfile(agg_path):
        return None
    return pd.read_csv(agg_path)


def plot_acc(dataset, noise, model="simplecnn"):
    noise_dir = f"noise_{noise}"
    base = os.path.join(RESULTS_ROOT, dataset, model, noise_dir)
    if not os.path.isdir(base):
        return None

    fig, ax = plt.subplots(figsize=(6, 4))
    for opt in OPT_ORDER:
        opt_path = os.path.join(base, opt)
        df = load_agg(opt_path)
        if df is None:
            continue
        ax.plot(df["Epoch"], df["Test Acc Mean"], label=opt, linewidth=2)
        ax.fill_between(
            df["Epoch"],
            df["Test Acc Mean"] - df["Test Acc Std"],
            df["Test Acc Mean"] + df["Test Acc Std"],
            alpha=0.2,
        )

    ax.set_xlabel("Epoch")
    ax.set_ylabel("Test Accuracy (%)")
    ax.set_title(f"{dataset} / {model} (noise={noise})")
    ax.legend()
    ax.grid(True)
    fig.tight_layout()

    out_path = os.path.join(FIG_DIR, f"{dataset.lower()}_noise{noise}_acc_mean_std.pdf")
    fig.savefig(out_path)
    plt.close(fig)
    return out_path


def main():
    outputs = []
    for dataset in ["CIFAR10", "CIFAR100"]:
        for noise in ["0.0", "0.05", "0.1"]:
            out = plot_acc(dataset, noise)
            if out:
                outputs.append(out)
    if outputs:
        print("Saved acc figures:")
        for o in outputs:
            print(o)
    else:
        print("No figures generated (missing aggregated stats?)")


if __name__ == "__main__":
    main()

import os
import sys
import glob
import json
import numpy as np
import pandas as pd

# Lightweight reimplementation of aggregate_runs_and_save to avoid torch import.
def aggregate_runs_and_save(folder):
    pattern = os.path.join(folder, 'run_*.csv')
    files = sorted(glob.glob(pattern))
    if not files:
        raise FileNotFoundError(f"No run CSV files found in {folder}")

    runs = [pd.read_csv(f) for f in files]

    final_accs = [float(r['Test Acc'].iloc[-1]) for r in runs]
    best_accs = [float(r['Test Acc'].max()) for r in runs]

    summary = {
        'num_runs': len(runs),
        'final_test_acc_mean': float(np.mean(final_accs)),
        'final_test_acc_std': float(np.std(final_accs, ddof=1)),
        'best_test_acc_mean': float(np.mean(best_accs)),
        'best_test_acc_std': float(np.std(best_accs, ddof=1)),
    }

    summary_path = os.path.join(folder, 'aggregated_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)

    acc_arrays = [r['Test Acc'].values for r in runs]
    max_len = max(len(a) for a in acc_arrays)
    acc_mat = np.zeros((len(acc_arrays), max_len), dtype=float)
    for i, a in enumerate(acc_arrays):
        acc_mat[i, :len(a)] = a

    epoch_idx = list(range(1, max_len + 1))
    mean = acc_mat.mean(axis=0)
    std = acc_mat.std(axis=0)
    df_epoch = pd.DataFrame({'Epoch': epoch_idx, 'Test Acc Mean': mean, 'Test Acc Std': std})
    df_epoch.to_csv(os.path.join(folder, 'aggregated_epoch_stats.csv'), index=False)

    excel_path = os.path.join(folder, 'runs_and_aggregate.xlsx')
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        for idx, r in enumerate(runs):
            sheet = f'run_{idx+1}'
            r.to_excel(writer, sheet_name=sheet, index=False)
        df_epoch.to_excel(writer, sheet_name='aggregate', index=False)

    return summary_path, excel_path


ROOT = os.path.join(os.path.dirname(__file__), 'results')

if not os.path.isdir(ROOT):
    sys.exit('results folder not found: {}'.format(ROOT))

updated = []
for dirpath, dirnames, filenames in os.walk(ROOT):
    if not any(f.startswith('run_') and f.endswith('.csv') for f in filenames):
        continue
    rel = os.path.relpath(dirpath, ROOT)
    parts = rel.split(os.sep)
    if len(parts) < 4:
        continue
    dataset, model, noise_dir, optimizer = parts[:4]
    if not noise_dir.startswith('noise_'):
        continue
    aggregate_runs_and_save(dirpath)
    updated.append((dataset, model, noise_dir, optimizer))
    print(f'Regenerated aggregates for {dataset}/{model}/{noise_dir}/{optimizer}')

if not updated:
    print('No run_* CSV folders found; nothing regenerated')
else:
    print(f'Total regenerated: {len(updated)}')

import os
import glob
import json
import numpy as np
import pandas as pd

RESULTS_ROOT = os.path.join(os.path.dirname(__file__), 'results')

if not os.path.isdir(RESULTS_ROOT):
    raise SystemExit(f'results folder not found: {RESULTS_ROOT}')

summary_stats = {}

for dataset in os.listdir(RESULTS_ROOT):
    dataset_path = os.path.join(RESULTS_ROOT, dataset)
    if not os.path.isdir(dataset_path):
        continue

    for model in os.listdir(dataset_path):
        model_path = os.path.join(dataset_path, model)
        if not os.path.isdir(model_path):
            continue

        ds_model_key = f'{dataset}|{model}'

        for noise_dir in os.listdir(model_path):
            if not noise_dir.startswith('noise_'):
                continue
            noise_path = os.path.join(model_path, noise_dir)
            if not os.path.isdir(noise_path):
                continue

            noise_value = noise_dir.replace('noise_', '')

            for optimizer in os.listdir(noise_path):
                opt_path = os.path.join(noise_path, optimizer)
                if not os.path.isdir(opt_path):
                    continue

                run_files = sorted(glob.glob(os.path.join(opt_path, 'run_*.csv')))
                if not run_files:
                    continue

                runs = [pd.read_csv(f) for f in run_files]
                final_accs = [float(r['Test Acc'].iloc[-1]) for r in runs]
                best_accs = [float(r['Test Acc'].max()) for r in runs]

                stats_key = f"{dataset}|noise_{noise_value}|{optimizer}"
                summary_stats[stats_key] = {
                    'final_mean': float(np.mean(final_accs)),
                    'final_std': float(np.std(final_accs, ddof=0)),
                    'best_mean': float(np.mean(best_accs)),
                    'best_std': float(np.std(best_accs, ddof=0)),
                    'num_runs': len(runs),
                }

                if ds_model_key not in summary_stats:
                    summary_stats[ds_model_key] = {}
                if 'noise_table' not in summary_stats[ds_model_key]:
                    summary_stats[ds_model_key]['noise_table'] = {}
                if optimizer not in summary_stats[ds_model_key]['noise_table']:
                    summary_stats[ds_model_key]['noise_table'][optimizer] = []
                summary_stats[ds_model_key]['noise_table'][optimizer].append({
                    'noise': noise_value,
                    'final_mean': summary_stats[stats_key]['final_mean'],
                    'final_std': summary_stats[stats_key]['final_std'],
                })

# Save summary_statistics.csv in the same format used by save_multirun_summary
summary_df = pd.DataFrame.from_dict(summary_stats, orient='index')
summary_csv_path = os.path.join(RESULTS_ROOT, 'summary_statistics.csv')
summary_df.to_csv(summary_csv_path)
print(f'Regenerated summary statistics at {summary_csv_path}')

